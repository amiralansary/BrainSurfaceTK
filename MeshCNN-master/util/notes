48735 edges at red50

before testing move all the train related testoutput files away first, they will be overwritten

gpu01 - twogroups
gpu03 - ready
gpu05 - largergroups
gpu08 - plateau more features
gpu11 - static low LR
gpu21 - ep50 linear run, second run

cluster - static morefeatures
cluster - 120ep cosine

next
hold all GPUs, make one run with the "best config" and if that is ok, run everything else
once have the good number of groups, rerun plateau and cosine with that
plateau with lower bound
find a good LR setting and then rerun big architecture

rerun age regression on both havles using best config - more data reduces var error
native runs
inflated run
merged
BATCH SIZE 1 vs BATCH SIZE 2




looking at loss plots it seems that train and test move in sync, so overfitting does not seem to be the problem
-> lr, overshooting

